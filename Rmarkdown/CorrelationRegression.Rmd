---
title: "Correlation and Regression"
author: "Alan T. Arnholt"
date: 'Last updated: `r format(Sys.time(), "%B %d, %Y at %X")`'
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE, 
                      message = FALSE, fig.align = "center")
library(scales)
```

# Correlation

## Some Data

```{r}
set.seed(13)
x <- 1:20
y <- x + rnorm(20, 0, 3)
DFP <- data.frame(x = x, y = y)
y <- x + rnorm(20, 0, 3)
DFN <- data.frame(x = -x, y = y)
rm(x, y)
library(tidyverse)
ggplot(data = DFP, aes(x = x, y = y)) + 
  geom_point(color = "blue") + 
  theme_bw() -> p1
p1
ggplot(data = DFN, aes(x = x, y = y)) + 
  geom_point(color = "blue") + 
  theme_bw() -> p2
p2
```

```{r}
library(patchwork)
p1/p2
p1 + p2
```



$$r = \dfrac{1}{n-1}\sum_{i=1}^n\frac{(x - \bar{x})}{s_x}\frac{(y - \bar{y})}{s_y}$$

```{r}
DFP %>% 
  mutate(x_xbar = x - mean(x), y_ybar = y - mean(y), zx = x_xbar/sd(x), zy = y_ybar/sd(y), zxzy = zx*zy) -> steps
steps
steps %>% 
  summarize(COR = sum(zxzy)/(length(x) -1), cor(x,y))
```

```{r}
DFN %>% 
  summarize(r = cor(x, y))
```

## Least Squares Regression


$$b_1 = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2} = r\cdot \frac{s_y}{s_x}$$

$$b_0 = \bar{y} - b_1\cdot\bar{x}$$

$$e_i = y_i - \hat{y_i}$$

$$\hat{y_i} = b_0 + b_1\cdot x_i$$

```{r}
DFP %>% 
  mutate(b1 = cor(x, y)*sd(y)/sd(x), b0 = mean(y)-b1*mean(x), yhat = b0 + b1*x, e = y - yhat) -> ans1
ans1
ans1 %>% 
  summarize(SSE = sum(e^2))
```

```{r}
mod <- lm(y ~ x, data = DFP)
summary(mod)
anova(mod)
anova(mod)[2, 2] -> SSE
SSE
anova(mod)[2, 3] -> MSE
MSE
RSE <- sqrt(MSE)
RSE
summary(mod)$sigma
```

## Adding least squares line to scatterplot

```{r}
ggplot(data = DFP, aes(x = x, y = y)) + 
  geom_point(color = "lightblue") + 
  theme_bw() + 
  geom_smooth(method = "lm", se = FALSE, color = "red")
```

```{r}
summary(mod)
coefficients(summary(mod)) -> STUFF
STUFF
b0 <- STUFF[1, 1]
b1 <- STUFF[2, 1]
c(b0, b1)
coef(mod)
```

## Using `moderndive` wrapper functions

* `get_regression_table()`
* `get_regression_points()`


```{r}
library(moderndive)
get_regression_table(mod) -> OUT 
OUT 
OUT$estimate
```

```{r}
get_regression_points(mod)
```

## Prediction

What does the `mod` predict for an `x` value of 15?

```{r}
yhat <- b0 + b1*15
yhat
# or
predict(mod, newdata = data.frame(x = 15))
#
predict(mod, newdata = data.frame(x = 15), interval = "conf", level = 0.95)
```

```{r}
ggplot(data = DFP, aes(x = x, y = y)) + 
  theme_bw() + 
  geom_smooth(method = "lm", color = "lightblue") + 
  geom_point(color = "purple")
```

## You try

Use the data set `VIT2005` from the `PASWR2` package to:

1. Create a scatterplot of `totalprice` versus `area`.

```{r}
library(PASWR2)
ggplot(data = VIT2005, aes(x = area, y = totalprice)) + 
  geom_point(color = "gold") + 
  theme_bw() -> p1
p1
```

___________

2. Regress `totalprice` onto `area` and store the resulting model in an object named `mod2`.  

```{r}
# Your code here
mod2 <- lm(totalprice ~ area, data = VIT2005)
summary(mod2)
```

_________

3. Add the least squares regression line to the scatterplot created in problem 1.

```{r}
p1 + 
  geom_smooth(method = "lm", color = "purple")
```


4. Use `mod2` to predict the price of an apartment that has 100 square meters.

```{r}
get_regression_table(mod2)
get_regression_points(mod2, newdata = data.frame(area = 100))
get_regression_points(mod2, newdata = data.frame(area = 100)) %>% 
  pull(totalprice_hat) -> yhat
yhat
predict(mod2, newdata = data.frame(area = 100))
predict(mod2, newdata = data.frame(area = 100)) -> yhat2
yhat2
#####
coef(mod2)
coef(mod2)[1] + coef(mod2)[2]*100 -> yhat3
yhat3
#####
summary(mod2)$coef
summary(mod2)$coef[1, 1] -> b0
summary(mod2)$coef[2, 1] -> b1
yhat4 <- b0 + b1*100
yhat4
c(yhat, yhat2, yhat3, yhat4)
```

------------------

5.  Interpret the slope for `mod2`.

For every 1 square meter increase in `area`, there is an associated increase of, on average, € `r comma(b1)` in `totalprice`.

------------------

6.  Create a 95% confidence interval for the mean price of apartments with 160 square meters.

```{r}
predict(mod2, newdata = data.frame(area = 160), interval = "conf", level = 0.95) -> CI
CI
```

A 95% confidence interval for $E(Y_h)$ is from € `r comma(CI[2])` to € `r comma(CI[3])`.

_________________

6.  Use the `GRADES` dataframe from the `PASWR2` package to develop a model that predicts GPA based on SAT score.  Compute the expected GPA score for an SAT score of 1300.

```{r}
ggplot(data = GRADES, aes(x = sat, y = gpa)) + 
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE) -> p2
p2
mod3 <- lm(gpa ~ sat, data = GRADES)
summary(mod3)
predict(mod3, newdata = data.frame(sat = 1300))
```

___________


```{r}
library(moderndive)
head(evals)
evals %>% 
  group_by(rank) %>% 
  summarize(Mean = mean(score))
```

What are the possible values for $\widehat{score}$ with this model?

```{r}
modR <- lm(score ~ rank, data = evals)
get_regression_points(modR) 
get_regression_points(modR) %>% 
  select(score_hat) %>% 
  unique() %>% 
  pull() -> myvalues
myvalues
```
The possible values for $\widehat{score}$ are `r myvalues[1:2]`, and `r myvalues[3]`.






```{r}
# Interaction Model
ggplot(data = evals, aes(x = age, y = score, color = gender)) + 
  geom_point() + theme_bw() +
  geom_smooth(method = "lm", se = FALSE) -> pINT
pINT
#
modINT <- lm(score ~ age + gender + age:gender, data = evals)
summary(modINT)

b0M <- coef(modINT)[1] + coef(modINT)[3]
b1M <- coef(modINT)[2] + coef(modINT)[4]
b0F <- coef(modINT)[1]
b1F <- coef(modINT)[2]
c(b0M, b1M, b0F, b1F)
pINT + 
  geom_abline(intercept = b0M, slope = b1M, color = "blue") + 
  geom_abline(intercept = b0F, slope = b1F, color = "pink")
```

## Parallel slopes model

```{r}
ggplot(data = evals, aes(x = age, y = score, color = gender)) + 
  geom_point() + 
  geom_parallel_slopes(se = FALSE) + 
  theme_bw() -> pPar
pPar
modP <- lm(score ~ age + gender, data = evals)
summary(modP)
b0M <- coef(modP)[1] + coef(modP)[3]
b0F <- coef(modP)[1]
b1M <- coef(modP)[2]
b1F <- coef(modP)[2]
#
pPar + 
  geom_abline(intercept = b0M, slope = b1M, color = "blue") + 
  geom_abline(intercept = b0F, slope = b1F, color = "pink")


```
```{r}
# Interaction Model
ggplot(data = evals, aes(x = age, y = score, color = rank)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw()
```

```{r}
# Parallel Slopes Model
ggplot(data = evals, aes(x = age, y = score, color = rank)) + 
  geom_point() + 
  geom_parallel_slopes(se = FALSE) + 
  theme_bw()
```


## Two approaches

```{r}
evals %>% 
  group_by(pic_color) %>% 
  summarize(M = mean(score))
#
modCP <- lm(score ~ pic_color, data = evals)
summary(modCP)
MBW <- coef(modCP)[1]
MCP <- coef(modCP)[1] + coef(modCP)[2]
c(MBW, MCP)
```

____________

## Your Turn:

Create a scatterplot of `score` versus `bty_avg` while mapping `gender` to color.  Would you use an interaction model or a parallel slopes model?  Add the appropriate model to your scatterplot. Write the equations for the two lines in your scatterplot. 

----------------

```{r}
mod4 <- lm(score ~ bty_avg + gender, data = evals)
mod5 <- lm(score ~ bty_avg + gender + bty_avg:gender, data = evals)
summary(mod4)
summary(mod5)
```
____________________

```{r}
ggplot(data = evals, aes(x = bty_avg, y= score, color = gender)) + 
  geom_point() + 
  geom_parallel_slopes(se = FALSE) + 
  theme_bw()
```

```{r}
ggplot(data = evals, aes(x = bty_avg, y= score, color = gender)) + 
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) + 
  theme_bw()
```





