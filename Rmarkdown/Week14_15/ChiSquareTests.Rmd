---
title: "STT 3850 : Weeks 14 and 15"
author: "Spring 2024"
institute: "Appalachian State University"
output: 
 beamer_presentation:
   theme: "Madrid"
   colortheme: "orchid"
   fonttheme: "professionalfonts"
   keep_tex: true
number_sections: true
fig_caption: yes
biblio-style: apalike
link-citations: TRUE
header-includes:
   - \usepackage{placeins}
   - \usepackage{color}
   - \usepackage{bm}
   - \usepackage{amsmath}
   - \usepackage{algorithm}
   - \usepackage[]{algpseudocode}
   - \usepackage{tabularx}
   - \usepackage{multirow}
   - \usepackage[most]{tcolorbox}
   - \usepackage{tikz}
   - \usepackage{lipsum}
   - \usepackage{mathtools}
   - \usepackage{actuarialangle}
   - \usepackage{multirow, longtable, array, dcolumn}
   - \usepackage{tabu}
   - \newcommand{\sdt}{\bullet}
   - \newcommand{\tss}{\textsuperscript}
   - \newcommand{\morearraysp}{\setlength{\arraycolsep}{2mm}}
   - \newcommand{\smarraysp}{\setlength{\arraycolsep}{1mm}}
   - \newcommand{\oldarraysp}{\setlength{\arraycolsep}{1.5pt}}
   - \newcommand{\matrixstretch}{\setlength{\extrarowheight}{4pt}}
   - \newcommand{\matrixnostretch}{\setlength{\extrarowheight}{0pt}}
   - \newcommand{\gil}[1]{\textrm{\gilfont{#1}}\normalfont }
   - \newfont{\gilfont}{msbm10 scaled 1000}
   - \newcommand{\DOT}{\usebox{\biggercirc}}
   - \newcommand{\pv}{\wp\text{-value}}

---

```{r, MEDskip, echo = FALSE}
library(knitr)
knit_hooks$set(document = function(x){
gsub("\\begin{tabular}", "\\medskip{}\\begin{tabular}", x, fixed = TRUE)
})
```



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE, message = FALSE, fig.align = 'center')
```


# Chi-Square Goodness-of-Fit Tests

## Background

- Many statistical procedures require knowledge of the population from which the sample is taken. For example, using Student's $t$-distribution for testing a hypothesis or constructing a confidence interval for $\mu$ assumes that the parent population is normal. 

- **Goodness-of-fit** (GOF) procedures are presented that will help to identify the distribution of the population from which the sample is drawn. 

- The null hypothesis in a goodness-of-fit test is a statement about the form of the cumulative distribution.  When all the parameters in the null hypothesis are specified, the hypothesis is called **simple**. 

- Recall that in the event the null hypothesis does not completely specify all of the parameters of the distribution, the hypothesis is said to be **composite**.  

## Background

- Goodness-of-fit tests are typically used when the form of the population is in question.  In contrast to most of the statistical procedures discussed so far, where the goal has been to **reject** the null hypothesis, in a GOF test one hopes to **retain** the null hypothesis.

- Given a single random sample of size $n$ from an unknown population
$F_X$, one may wish to test the hypothesis that $F_X$ has some known
distribution $F_0(x)$ for all $x$.  

## Background

- For example, using the data frame `SOCCER` from the `PASWR2` package, is it reasonable to assume the number of goals scored during regulation time for the 232 soccer matches has a Poisson distribution with $\lambda=2.5$?

- Before applying the chi-square goodness-of-fit test, the data must be grouped according to some scheme to form $k$ mutually exclusive categories.  When the null hypothesis completely specifies the population, the probability that a random observation will fall into each of the chosen or fixed categories can be computed. 

## Background

- Once the probabilities for a data point to fall into each of the chosen or fixed categories is computed, multiplying the probabilities by $n$ produces the expected counts for each category under the null distribution.  

- If the null hypothesis is true, the differences between the counts observed in the $k$ categories and the counts
expected in the $k$ categories should be small.  


## Background

- The test criterion for testing $H_0: F_X(x) = F_0(x) \text{ for all } x$ against the alternative $H_1: F_X(x) \ne F_0(x)  \text{ for some } x$ when the null hypothesis is completely specified is

\begin{equation}
\chi_{\text{obs}}^2=\sum_{i=1}^{k} \frac{(O_k - E_k)^2}{E_k},
\end{equation}

where $\chi_\text{obs}^2$ is the sum of the squared deviations between what is observed $(O_k)$ and what is expected $(E_k)$ in each of the $k$ categories divided by what is expected in each of the $k$ categories.  Large values of $\chi_\text{obs}^2$ occur when the observed data are inconsistent with the null hypothesis and thus lead to rejection of the null hypothesis. The exact distribution of $\chi_\text{obs}^2$ is very complicated; however, for large $n$, provided all expected categories are at least 5, $\chi_\text{obs}^2$ is distributed approximately $\chi^2$ with $k-1$ degrees of freedom.

## Background

- NOTE: When the null hypothesis is composite, that is, not all of the parameters  are specified, the degrees of freedom for the random variable  $\chi_\text{obs}^2$ are reduced by one for each parameter that must be estimated.

## Soccer Example

Test the hypothesis that the number of goals scored during regulation time for the 232 soccer matches stored in the data frame `SOCCER` has a Poisson `cdf` with $\lambda=2.5$ with the chi-square goodness-of-fit test and an $\alpha$ level of 0.05. Produce a histogram showing the number of observed goals scored during regulation time and superimpose on the histogram the number of goals that are expected to be made when the distribution of goals follows a Poisson distribution with $\lambda=2.5$.

## Soccer Solution

- Since the number of categories for a Poisson distribution is theoretically infinite, a table is first constructed of the observed number of goals to get an idea of reasonable categories.

```{r}
library(PASWR2)
xtabs(~goals, data = SOCCER)
```

## Soccer Solution

Based on the table, a decision is made to create categories for 0, 1, 2, 3, 4, 5, and 6 or more goals.  Under the null hypothesis that $F_0(x)$ is a Poisson distribution with $\lambda=2.5$, the probabilities of scoring 0, 1, 2, 3, 4, 5, and 6 or more goals are computed with `R` as follows:

```{r}
PX <- c(dpois(0:5, 2.5), ppois(5, 2.5, lower = FALSE))
PX[1:4] # Probabilities for categories 0, 1, 2, 3
PX[4:6] # Probabilities for categories 4, 5, and 6 or more
```

## Soccer Solution

\begin{tcolorbox}
Since there were a total of $n=232$ soccer games, the expected
number of goals for the six categories is simply $232 \times
\tt{PX}$.
\end{tcolorbox}

```{r}
EX <- 232*PX
OB <- c(as.vector(xtabs(~goals, data = SOCCER)[1:6]), 
        sum(xtabs(~goals, data = SOCCER)[7:9]))
OB
ans <- cbind(PX, EX, OB)
row.names(ans) <- c(" X=0"," X=1"," X=2", 
                    " X=3"," X=4"," X=5","X>=6")
```

## Soccer Solution

```{r}
ans
```

## Soccer Solution

The null and alternative hypotheses for using the chi-square goodness-of-fit test to test the hypothesis that
the number of goals scored during regulation time for the 232 soccer matches stored in the data frame `SOCCER` has a Poisson `cdf` with $\lambda=2.5$ are

\begin{align*}
        H_0&: F_X(x) = F_0(x) \sim Pois(\lambda=2.5)\text{ for all } x \text{ versus
        }\\
        H_1&:  F_X(x) \ne F_0(x) \text{ for some } x.
\end{align*}

## Soccer Solution

- The test statistic chosen is $\chi_{\text{obs}}^2.$

- Reject if $\chi^2_{\text{obs}}>\chi^2_{1-\alpha;k-1}$.

```{r}
chi.obs <- sum((OB-EX)^2/EX)
chi.obs
```

## Soccer Solution

```{r}
chisq.test(x = OB, p = PX)
```


## Soccer Solution

$`r chi.obs`=\chi^2_{\text{obs}}\overset{?}{>}\chi^2_{0.95;6}=`r qchisq(0.95, 6)`$.

The $p$-value is `r pchisq(chi.obs,6, lower = FALSE)`.

```{r}
p.val <- pchisq(chi.obs, 7-1, lower = FALSE)
p.val
```

## Soccer Solution

- Since $\chi^2_{\text{obs}}= `r chi.obs`$ is not greater than $\chi^2_{0.95;6}=`r qchisq(0.95, 6)`$, fail to reject $H_0$.

- Since the $p$-value = `r p.val` is greater than 0.05, fail to reject $H_0$.

## Soccer Solution

\begin{tcolorbox}
\textbf{English Conclusion:} There is no evidence to suggest that the true \textbf{cdf} does not equal the Poisson 
distribution with $\lambda=2.5$ for at least one $x$.
\end{tcolorbox}

## Soccer Solution

The following code can be used to create a histogram with superimposed expected goals.

```{r, eval = FALSE}
hist(SOCCER$goals, breaks = c((-0.5 + 0):(8 + 0.5)), 
     col = "lightblue", 
     xlab = "Goals scored", ylab = "", 
     freq = TRUE, main = "")
x <- 0:8
fx <- (dpois(0:8, lambda = 2.5))*232
lines(x, fx, type = "h")
lines(x, fx, type = "p", pch = 16)
```

## Soccer Solution

```{r, echo = FALSE, out.height = '65%', out.width = '85%'}
hist(SOCCER$goals, breaks = c((-0.5 + 0):(8 + 0.5)), 
     col = "lightblue", 
     xlab = "Goals scored", ylab = "", 
     freq = TRUE, main = "")
x <- 0:8
fx <- (dpois(0:8, lambda = 2.5))*232
lines(x, fx, type = "h")
lines(x, fx, type = "p", pch = 16)
```



## All Parameters Known

- Bansley et al. (1992) investigated the relationship between month of birth and achievement in sport. Birth dates were collected for players in teams competing in the 1990 World Cup soccer games.

```{r}
Observed <- c(150, 138, 140, 100)
names(Observed) <- c("Aug-Oct", "Nov-Jan", 
                     "Feb-April", "May-July")
Observed
```

## All Parameters Known

We wish to test whether these data are consistent with the hypothesis that birthdays of soccer players are uniformly distributed across the four quarters of the year. Let $P_i$ denote the probability of a birth occurring in the $i^{th}$ quarter; the hypotheses are as follows:

$H_0: p_1=\frac{1}{4}, p_2=\frac{1}{4}, p_3=\frac{1}{4}, p_4=\frac{1}{4}$ versus $H_A: p_i \neq \frac{1}{4}$ for at least one $i$.

There were a total of $n = 528$ players considered for this study, so the expected count for each quarter is $528/4 = 132$.


## All Parameters Known

$\chi^2_{obs} = \sum_{i=1}^k\frac{(O_i - E_i)^2}{E_i} = \frac{(150 - 132)^2}{132} + \frac{(138 - 132)^2}{132} + \frac{(140 - 132)^2}{132} + \frac{(100 - 132)^2}{132} = 10.97$

```{r}
(chi_obs <- sum((Observed - 132)^2/132))
# Or
chisq.test(Observed, p = c(1/4, 1/4, 1/4, 1/4))$stat
```
## All Parameters Known

```{r}
chisq.test(Observed, p = c(1/4, 1/4, 1/4, 1/4)) -> CST
CST
CST$observed
CST$expected
```

## All Parameters Known

```{r}
(pvalue <- pchisq(CST$stat, 3, lower = FALSE))
# Or
CST$p.value
```

## All Parameters Known - Conclusion

\begin{tcolorbox}
Given the $p-value$ of $0.012$ evidence suggests birthdays for World Cup soccer players are not uniformly distributed.
\end{tcolorbox}

## All Parameters Known - Example 2

Suppose you draw 100 numbers at random from an unknown distribution.  Thirty values fall in the interval $(0, 0.25]$, 30 fall in $(0.25, 0.75]$, 22 fall in $(0.75, 1.25]$, and the rest fall in $(1.25, \infty]$.  Your friend claims that the distribution is exponential with parameter $\lambda = 1$.  Do you believe her?

* A random variable $X$ has the exponential distribution with parameter $\lambda > 0$ if its **pdf** is

$$f(x) = \lambda e^{-\lambda x},\quad x \geq 0.$$

## All Parameters Known - Example 2

We wish to test the following:

\begin{tcolorbox}
$H_0:$ The data are from an exponential distribution with $\lambda = 1$.

$H_A:$ The data are not from an exponential distribution with $\lambda = 1$.
\end{tcolorbox}

## All Parameters Known - Example 2

Given $X \sim \text{Exp}(\lambda = 1)$.  The probabilities for each interval are as follows:

$p_1 = P(0 \leq X \leq 0.25)=\int_0^{0.25}e^{-x}\,dx =`r pexp(0.25, 1)`$

$p_2 = P(0.25 \leq X \leq 0.75)=\int_{0.25}^{0.75}e^{-x}\,dx =`r pexp(0.75, 1) - pexp(0.25, 1)`$

$p_3 = P(0.75 \leq X \leq 1.25)=\int_{0.75}^{1.25}e^{-x}\,dx =`r pexp(1.25, 1) - pexp(0.75, 1)`$

$p_4 = P(1.25 \leq X \leq \infty)=\int_{1.25}^{\infty}e^{-x}\,dx =`r pexp(1.25, 1, lower = FALSE)`$

## All Parameters Known - Example 2

```{r}
p1 <- pexp(0.25, 1)
p2 <- pexp(0.75, 1) - pexp(0.25, 1)
p3 <- pexp(1.25, 1) - pexp(0.75, 1)
p4 <- pexp(1.25, 1, lower = FALSE)
ps <- c(p1, p2, p3, p4)
ps
```

## All Parameters Known - Example 2

```{r}
EXP <- ps*100
EXP
OBS <- c(30, 30, 22, 18)
test_stat <- sum((OBS - EXP)^2/EXP)
test_stat
```

## All Parameters Known - Example 2

```{r}
# Another approach
chisq.test(OBS, p = ps)
pvalue <- chisq.test(OBS, p = ps)$p.value
pvalue
```

## All Parameters Known - Example 2 - Conclusion

\begin{tcolorbox}
If you test using $\alpha = 0.05$, you will fail to reject the null hypothesis since the $p-value$ $= 0.0599 > \alpha = 0.05$.  There is not convincing evidence that the data do not come from an Exp($\lambda = 1$).
\end{tcolorbox}


# Categorical Data

## Different Scenarios

The $2 \times 2$ contingency table can be generalized for $I$ rows and $J$
columns and is referred to as an $I \times J$ contingency table. The sampling
scheme employed to acquire the information in the table will determine the type
of hypothesis that can be tested.  Consider the following two scenarios:

## Scenario One:

SCENARIO ONE:  Is there an association between gender and a person's happiness? To investigate whether happiness depends on gender, one might use information collected from the General Social Survey (GSS) (\href{http://sda.berkeley.edu/GSS}{http://sda.berkeley.edu/GSS}). In each survey, the GSS asks, ``Taken all together, how would you say things are these days --- would you say that you are very happy, pretty happy, or not too happy?'' Respondents to each survey are coded as either male or female. The information in the next slide shows how a subset of respondents (26-year-olds) were classified with respect to the variables HAPPY and SEX.

## Scenario One:

```{r}
HA <- c(110, 277, 50, 163, 302, 63)
HAT <- matrix(data = HA, nrow = 2, byrow = TRUE)
dimnames(HAT) <- list(SEX = c("Male", "Female"),
 Category = c("Very Happy", "Pretty Happy", "Not To Happy"))
HAT
```

## Scenario One - Expected Values

```{r}
E <- outer(rowSums(HAT), colSums(HAT), "*")/sum(HAT)
E
# OR
chisq.test(HAT)$expected
```

## Scenario Two

SCENARIO TWO:  In a double blind randomized drug trial (neither the patient nor the physician evaluating the patient knows the treatment, drug or placebo, the patient receives), 400 male patients with mild dementia were randomly divided into two groups of 200.  One group was given a placebo over three months while the second group received an experimental drug for three months.  At the end of the three months, the physicians (all  psychiatrists) classified the 400 patients into one of three categories: improved, no change,  or worse.   The information on the next slide shows how the pschiatrists classified the patients.  Are the proportions in the three status categories  the same for the two treatments?

## Scenario Two

```{r}
DT <- c(67, 76, 57, 48, 73, 79)
DTT <- matrix(data = DT, nrow = 2, byrow = TRUE)
dimnames(DTT) <- list(Treatment = c("Drug", "Placebo"),
   Category = c("Improve", "No Change", "Worse"))
DTT
```

## Scenario Two - Expected Values

```{r}
E <- chisq.test(DTT)$expected
E
```

## Categorical Data

The two scenarios illustrate two different sampling schemes that both result in $I \times J$ contingency tables.  In the first scenario, there is a single population (Americans) and individuals are sampled from this single population and classified into one of the $IJ$ cells of the $I\times J$ contingency table based on the $I=2$ SEX categories and the $J=3$  HAPPY categories.  The format of an $I \times J$ contingency table when sampling from a single population is shown in Table \ref{CTtable}.  The number of observations from the $i$\tss{th} row classified into the $j$\tss{th} column is denoted by $n_{ij}$. It follows that the number of observations in the $j$\tss{th} column $(1 \le j \le J)$ is $n_{\sdt j}=n_{1j}+n_{2j}+\dots+n_{Ij}$, while the number of observations in the $i$\tss{th} row $(1 \le i \le I)$ is $n_{i\sdt}=n_{i1}+n_{i2}+\dots+n_{iJ}$.

## Categorical Data

\begin{table}[!ht]
\caption{Contingency table when sampling from a single population\label{CTtable}}
\medskip
\centerline{$\morearraysp
\begin{array}{|l|*{4}{c}|c|}
\hline
&    \text{Col } 1 &  \text{Col }2&   \cdots &  \text{Col }J&   Totals\\
\hline
\text{Row }1  & n_{11} & n_{12}  &\cdots&        n_{1J} & n_{1\sdt}\\
\text{Row }2  & n_{21} & n_{22}  &\cdots&        n_{2J} & n_{2\sdt}\\
\vdots &       \vdots &  \vdots &     &          \vdots &\vdots\\
\text{Row }I  & n_{I1} & n_{I2} &\cdots&         n_{IJ} & n_{I\sdt}\\ \hline
\text{Totals }& n_{\sdt 1}  & n_{\sdt 2}  &\cdots&         n_{\sdt J}  & n\\
\hline
\end{array}$\oldarraysp}
\end{table}

## Categorial Data

The true population proportion of individuals in cell $(i, j)$ will be denoted $\pi_{ij}$. Under the assumption of independence between row and column variables (SEX and HAPPY in this example), $\pi_{ij}=\pi_{i\sdt} \times \pi_{\sdt j}$, where $\pi_{i\sdt}=\sum_{j=1}^{J} \pi_{ij}$ and $\pi_{\sdt j}=\sum_{i=1}^{I} \pi_{ij}$.  That is,  $\pi_{i\sdt}$ is the proportion of observations in the population classified in category $i$ of the row variable and $\pi_{\sdt j}$ is the proportion of observations in the population classified in category $j$ of the column variable.  Since $\pi_{i\sdt}$ and $\pi_{\sdt j}$ are marginal population proportions, it follows that $\hat\pi_{i\sdt}=p_{i\sdt}=\frac{n_{i\sdt}}{n}$ and  $\hat\pi_{\sdt j}=p_{\sdt j}=\frac{n_{\sdt j}}{n}$, where $n$ is the sample size.  Under the assumption of independence the expected count for cell $(i, j)$ is $\mu_{ij}= n \pi_{ij}=n \pi_{i\sdt}\pi_{\sdt j}$ and $\hat\mu_{ij}=n\hat\pi_{ij} = n\hat\pi_{i\sdt} \hat\pi_{\sdt j} = n \frac{n_{i\sdt}}{n}\frac{n_{\sdt j}}{n} = \frac{n_{i\sdt}n_{\sdt j}}{n}.$

## Categorical Data

In the second scenario, there are two distinct populations from which samples are taken.  The first population is the group of all patients receiving the experimental drug while the second population is the group of all patients receiving a placebo.  In this scenario, there are $I=2$ separate populations and $J=3$ categories for the $I=2$ populations.  Individuals sampled from the $I=2$ distinct populations are classified into one of the $J=3$ status categories.  This scenario has fixed row totals whereas the first scenario does not.  In the first scenario, only the total sample size, $n$, is fixed.  That is, neither the row nor the column totals are fixed.  This is in contrast to scenario two, where the number of patients in each treatment group (row) was fixed.  The notation used for an $I \times J$ contingency table when $I$ samples from $I$ distinct populations differs slightly from the notation used in  Table \ref{CTtable} with a contingency table from a single sample.

## Categorical Data

Since the sample sizes of the $I$ distinct populations are denoted $n_{i\sdt}$, the total for all $I$ samples is denoted by $n_{\sdt\sdt}$ rather than the notation $n$ used for a single sample in Table \ref{CTtable}.   Table \ref{CTRP} shows the general form and notation used for an $I \times J$ contingency table when sampling from $I$ distinct populations.  Each observation in each sample is classified into one of $J$ categories.  If $n_{i\sdt}$ denotes the number of observations in the $i$\tss{th} sample $(1 \le i \le I)$ and $n_{ij}$ denotes the number of observations from the $i$\tss{th} sample classified into the $j$\tss{th} category $(1 \le j \le J)$, it follows that the number of observations in the $j$\tss{th} column is $n_{\sdt j}=n_{1j}+n_{2j}+\dots+n_{Ij}$, while the number of observations in the $i$\tss{th} row is 
$n_{i\sdt}=n_{i1}+n_{i2}+\dots+n_{iJ}$.

## Categorical Data

\begin{table}[!ht]\caption{General form and
notation used for an $I \times J$ contingency table when sampling from $I$
distinct populations\label{CTRP}}
\medskip
\matrixstretch
\morearraysp
% \vspace{2ex}
\centerline{
$
\begin{array}{|l|*{4}{c}|c|}
\hline
&\text{Category }1&\text{Category }2&\cdots&\text{Category }J&\text{Totals}\\ \hline
\text{Population }1&n_{11}&n_{12}&\cdots&n_{1J}&n_{1\sdt}\\
\text{Population } 2&n_{21}&n_{22}&\cdots&n_{2J}&n_{2\sdt}\\
\hfill\vdots\hfill&\vdots&\vdots&&\vdots&\vdots\\
\text{Population } I&n_{I1}&n_{I2}&\dotsc&n_{IJ}&n_{I\sdt}\\
\hline
\text{Totals}&n_{\sdt 1}&n_{\sdt 2}&\dotsc&n_{\sdt J}&n_{\sdt\sdt}\\
\hline
\end{array}
$}\matrixnostretch\oldarraysp
\end{table}


# Chi-Square Tests of Independence

## Scenario One

Scenario one asks if there is an association between gender and a person's happiness.  Recall that two events, $A$ and $B$, were defined as independent when $\gil{P}(A \cap B)=\gil{P}(A)\times \gil{P}(B)$ or, equivalently, when $\gil{P}(A|B)=\gil{P}(A)$. If, instead of having a random sample from a single population, an $I \times J$ contingency table consisted of entries from the population, association could be mathematically verified by showing that $\gil{P}(n_{ij}) \ne \gil{P}(n_{i\sdt}) \times \gil{P}(n_{\sdt j})$ for some $i$ and $j$.  If by chance  $\gil{P}(n_{ij}) = \gil{P}(n_{i\sdt}) \times \gil{P}(n_{\sdt j})$  for all $i$ and $j$, then one would conclude there is no association between gender and a person's happiness.  

## Scenario One

That is, the variables gender and happiness would be considered mathematically independent. Since the entire population is not given but rather a sample from a population, the values in the $I \times J$ contingency table can be expected to change from sample to sample. The question is, ``By how much can the variables deviate from the mathematical definition of independence and still be consideredstatistically independent?''

## Scenario One

The null and alternative hypotheses to test for independence between row and
column variables is written $H_0: \pi_{ij}=\pi_{i\sdt}\pi_{\sdt j}$ versus
$H_1:\pi_{ij} \ne \pi_{i\sdt}\pi_{\sdt j}$.   The  test statistic is
\begin{equation}\label{ChiSqStatIndep} \chi_{\text{obs}}^2
=\sum_{i=1}^I\sum_{j=1}^J \frac{(O_{ij} - E_{ij})^2}{E_{ij}}.
\end{equation}

## Scenario One

It compares the observed frequencies in the table with the expected frequencies when $H_0$ is true. Under the assumption of independence, and when the observations in the cells are sufficiently large (usually greater than 5), $\chi_{\text{obs}}^2 =\sum_{i=1}^{I}\sum_{j=1}^J \frac{(n_{ij}-\hat{\mu}_{ij})^2}{\hat{\mu}_{ij}} \overset{\sdt}{\sim} \chi^2_{(I-1)(J-1)},$ where $\hat{\mu}_{ij}=\frac{n_{i\sdt}n_{\sdt j}}{n} = E_{ij}$ and $n_{ij} = O_{ij}$.  The null hypothesis of independence is rejected when $\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)(J-1)}$.

## Scenario One

The chi-squared approximation is generally satisfactory if the $E_{ij}$s ($\hat{\mu}_{ij}$s) in the test statistic are not too small.  Various rules of thumb exist for what might be considered too small.  A very conservative rule is to require all $E_{ij}$s to be 5 or more.  This can be accomplished by combining cells with small $E_{ij}$s and reducing the overall degrees of freedom.  At times, it may be permissible to let the $E_{ij}$ of a cell be as low as 0.5.

## Test for Scenario One

**Hypotheses**---  $H_0: \pi_{ij} = \pi_{i\sdt }\pi_{\sdt j}$ (Row and column variables are independent.) versus $H_1: \pi_{ij} \ne \pi_{i\sdt }\pi_{\sdt j}$ for at least one $i, j$ (Row and column variables are dependent.)

## Test for Scenario One

**Test Statistic** --- The test statistic is $$\chi_{\text{obs}}^2=\sum_{i=1}^I \sum_{j=1}^J \frac{(O_{ij}-E_{ij})^2}{E_{ij}} \overset{\sdt}{\sim} \chi^2_{(I-1)(J-1)}=\chi^2_{(2-1)(3-1)}=\chi^2_2$$ under the assumption of independence. The $\chi^2_{\text{obs}}$ value is $4.3215$.

## Test for Scenario One

**Rejection Region Calculations** --- The rejection region is
        $$\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)(J-1)}=\chi^2_{0.95;2}=
        `r qchisq(0.95, 2)`.$$  Before the statistic
        $\chi^2_{\text{obs}}= \sum_{i=1}^I\sum_{j=1}^J 
        \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ can be computed,
        the expected counts for each of the $ij$ cells must be calculated.  
        Note that $O_{ij}=n_{ij}$ and
        $E_{ij}=\frac{n_{i\sdt}n_{\sdt j}}{n}$.
 
## Test for Scenario One

**Rejection Region Calculations**

```{r}
(E <- chisq.test(HAT)$expected)
```

$$\chi^2_{\text{obs}}=\frac{(110 - 123.6280)^2}{123.6280} +
\frac{(277-262.2)^2}{262.2}+ \dots +\frac{(63-61.828)^2}{61.828}=4.3215.$$


## Test for Scenario One

The value of the test statistic is $\chi^2_{obs}=4.3215$.
This can be done with code by entering

```{r}
chi.obs <- sum((HAT - E)^2/E )
chi.obs
```

$4.3215 = \chi^2_{\text{obs}} \overset{?}{>} \chi^2_{0.95,2}= `r qchisq(0.95, 2)`$.

## Test for Scenario One

**Statistical Conclusion**---The $\pv$ is 0.1152.

```{r}
p.val <- pchisq(chi.obs, 2, lower = FALSE)
p.val
```

- From the rejection region, since $\chi_{\text{obs}}^2=4.3215 < \chi^2_{0.95;2}=`r qchisq(0.95, 2)`$, fail to reject the null hypothesis of independence.

- Since the $\pv = `r p.val`$ is greater than 0.05, fail to reject the null hypothesis of independence.

- Fail to reject ${\bf H_0}$.


## Test for Scenario One

**English Conclusion**---There is not sufficient evidence to suggest the variables gender and happiness are statistically dependent.


The function `chisq.test()` can also be used to test the null hypothesis of independence.

```{r}
chisq.test(HAT)
```


## Example

```{r}
library(PASWR2)
(xtabs(~sex + survived, data = TITANIC3) -> T1)
chisq.test(T1, correct = FALSE) -> CST
CST
```
## Example

```{r}
(EXP <- CST$expected)
(OBS <- CST$observed)
(chi_obs <- sum((OBS - EXP)^2/EXP))
```


# Chi-Square Tests of Homogeneity

## Question

The question of interest in scenario two is whether the proportions in each of
the $j=3$ categories for the $i=2$ populations are equivalent.  Specifically, is
$\pi_{1j} = \pi_{2j}$ for all $j$?  This question is answered with a test of
homogeneity.  In general, the null hypothesis for a test of homogeneity with
$i=I$ populations is written
\begin{equation}
 H_0: \pi_{1j}=\pi_{2j} = \dots = \pi_{Ij}\text{ for all } j \text{ versus } H_1:
\pi_{ij} \ne \pi_{i+1, j} \: \text{ for some } (i, j).
\end{equation}

## Expressed in Words

- Expressed in words, the null hypothesis is that the $I$ populations are
homogeneous with respect to the $J$ categories versus the $I$ populations are
not homogeneous with respect to the $J$ categories.

## Equivalent Formulation

\begin{tcolorbox}
An equivalent interpretation is that for each population $i=1,2,\dots, I$, the
proportion of people in the $j\tss{th}$ category is the same.  When $H_0$ is
true, $\pi_{1j}=\pi_{2j}=\dots=\pi_{Ij}$ for all $j$.
\end{tcolorbox}

## Theory

Under the null hypothesis, $\mu_{ij}= n_{i\sdt} \pi_{ij}$, $\hat\pi_{ij}=p_{ij}= \frac{n_{\sdt j}}{n_{\sdt\sdt}}$, and $\hat{\mu}_{ij}= \frac{n_{i\sdt} n_{\sdt j}}{n_{\sdt\sdt}}=E_{ij}$.  When $H_0$ is true, all the probabilities in the $j$\tss{th} column are equal, and a pooled estimate of $\pi_{ij}$ is obtained by adding all the frequencies in the $j$\tss{th} column $(n_{\sdt j})$ and dividing the total by $n_{\sdt\sdt}$.  The statistic used in this type of problem  has the same form as the one used for the test of independence in \eqref{ChiSqStatIndep}.  

Substituting the homogeneity expressions for $O_{ij}$ and $E_{ij}$, the  statistic is expressed as

$$\chi_{\text{obs}}^2=\sum_{i=1}^I\sum_{j=1}^J \frac{(n_{ij} - n_{i\sdt} n_{\sdt
j}/n_{\sdt\sdt})^2}{n_{i\sdt} n_{\sdt j}/n_{\sdt\sdt}} \overset{\sdt}{\sim}
\chi^2_{(I-1)(J-1)}.$$ 

The null hypothesis of homogeneity is rejected when
$\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)(J-1)}$.

## Test for Scenario Two

**Hypotheses** ---  $H_0: \pi_{1j}=\pi_{2j} \text{ for all } j$ versus $H_1: \pi_{i,\, j} \ne \pi_{i+1,\, j}\text{ for some }(i, j)$. That is, all the probabilities in the same column are equal to each other versus at least two of the probabilities in the same column are not equal to each other.

## Test for Scenario Two

**Test Statistic**---The test statistic is
        $$\chi_{\text{obs}}^2=\sum_{i=1}^I \sum_{j=1}^J 
        \frac{(O_{ij}-E_{ij})^2}{E_{ij}} \sim \chi^2_{(I-1)(J-1)}=
        \chi^2_{(2-1)(3-1)}=\chi^2_2$$
        under the null hypothesis. The $\chi^2_{\text{obs}}$ value is
        $6.7584$.
        
## Test for Scenario Two

**Rejection Region Calculations**---The rejection region is
$$\chi_{\text{obs}}^2 > \chi^2_{1-\alpha; (I-1)\cdot(J-1)} = \chi^2_{0.95;2}=`r qchisq(0.95, 2)`.$$  

- Before the statistic $\chi^2_{\text{obs}}= \sum_{i=1}^I\sum_{j=1}^J \frac{(O_{ij} - E_{ij})^2}{E_{ij}}$ can be computed, the expected counts for each of the $ij$ cells must be determined.  
        
- Recall that $O_{ij}=n_{ij}$ and $E_{ij}=\frac{n_{i\sdt}n_{\sdt j}}{n_{\sdt\sdt}}.$

## The Data

 - Data will often come summarized in contingency tables.  

```{r}
DP <- c(67, 76, 57, 48, 73, 79)
MDP <- matrix(data = DP, nrow = 2, byrow = TRUE)
dimnames(MDP) <- list(Pop = c("Drug", "Placebo"), 
    Status = c("Improve", "No Change", "Worse"))
TDP <- as.table(MDP)
TDP
```

## Putting the data back in a tidy format

```{r}
library(tidyverse)
NT <- TDP %>% 
  tibble::as_tibble() %>% 
  uncount(n)
head(NT, 3)
```

## More Output for Scenario Two

```{r}
E <- chisq.test(TDP)$expected
E
```

$$\chi^2_{\text{obs}}=\frac{(67 - 57.5)^2}{57.5} + \frac{(76 - 74.5)^2}{74.5}+
\dots +\frac{(79-68)^2}{68}=6.7584.$$


## Test for Scenario Two

The value of the test statistic is $\chi^2_{obs}=6.7584.$. This can be done with code by entering

```{r}
chi.obs <- sum((TDP - E)^2/E )
chi.obs
```

$6.7584 = \chi^2_{\text{obs}}\overset{?}{>} \chi^2_{.95,2}= `r qchisq(0.95, 2)`.$

## Test for Scenario Two

**Statistical Conclusion**---The $\pv$ is 0.03408.

```{r}
p.val <- pchisq(chi.obs, 2, lower = FALSE)
p.val
```

- From the rejection region, since $\chi_{\text{obs}}^2=`r chi.obs` > \chi_{0.95;2} = `r qchisq(0.95, 2)`$, reject the null hypothesis of homogeneity.

- Since the $\pv = `r p.val`$ is less than 0.05, reject the null hypothesis of homogeneity.

## Test for Scenario Two

**English Conclusion**---There is sufficient evidence to suggest that not all of the probabilities for the $i=2$ populations with respect to each of the $J$ categories are equal.

Using `chisq.test()` directly produces the same results.

```{r}
chisq.test(TDP)
```

